{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9098dcb1",
      "metadata": {
        "id": "9098dcb1"
      },
      "source": [
        "## BERT for sequence classification\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VtlNYCOUiuO2"
      },
      "id": "VtlNYCOUiuO2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#may not be necessary for COLAB - uncomment if so next cell fails\n",
        "!pip install transformers datasets evaluate\n"
      ],
      "metadata": {
        "id": "R8zst-YJTtfe"
      },
      "execution_count": null,
      "outputs": [],
      "id": "R8zst-YJTtfe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2484167a",
      "metadata": {
        "id": "2484167a"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast, \\\n",
        "     DataCollatorWithPadding, pipeline\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ac898e",
      "metadata": {
        "id": "b6ac898e"
      },
      "outputs": [],
      "source": [
        "# for COLAB\n",
        "import requests\n",
        "\n",
        "URL = \"https://raw.githubusercontent.com/Tom-Niesytto/oreilly-hands-on-transformers/main/data/snips.train.txt\"\n",
        "response = requests.get(URL)\n",
        "snips_rows = response.text.splitlines()\n",
        "snips_rows[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e17957",
      "metadata": {
        "id": "e4e17957"
      },
      "outputs": [],
      "source": [
        "#for local run\n",
        "#snips_file = open('../data/snips.train.txt', 'rb')\n",
        "#snips_rows = snips_file.readlines()\n",
        "#snips_rows[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f7b8a0e",
      "metadata": {
        "id": "9f7b8a0e"
      },
      "outputs": [],
      "source": [
        "# This code segment parses the snips dataset into a more manageable format\n",
        "\n",
        "utterances = []\n",
        "tokenized_utterances = []\n",
        "labels_for_tokens = []\n",
        "sequence_labels = []\n",
        "\n",
        "utterance, tokenized_utterance, label_for_utterances = '', [], []\n",
        "for snip_row in snips_rows:\n",
        "    if len(snip_row) == 2:  # skip over rows with no data\n",
        "        continue\n",
        "    if ' ' not in snip_row.decode():  # we've hit a sequence label\n",
        "        sequence_labels.append(snip_row.decode().strip())\n",
        "        utterances.append(utterance.strip())\n",
        "        tokenized_utterances.append(tokenized_utterance)\n",
        "        labels_for_tokens.append(label_for_utterances)\n",
        "        utterance = ''\n",
        "        tokenized_utterance = []\n",
        "        label_for_utterances = []\n",
        "        continue\n",
        "    token, token_label = snip_row.decode().split(' ')\n",
        "    token_label = token_label.strip()\n",
        "    utterance += f'{token} '\n",
        "    tokenized_utterance.append(token)\n",
        "    label_for_utterances.append(token_label)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78793ee",
      "metadata": {
        "id": "c78793ee"
      },
      "outputs": [],
      "source": [
        "len(labels_for_tokens), len(tokenized_utterances), len(utterances), len(sequence_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e24417e",
      "metadata": {
        "id": "0e24417e"
      },
      "outputs": [],
      "source": [
        "utterances[0], sequence_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9084ab13",
      "metadata": {
        "id": "9084ab13"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774c7c64",
      "metadata": {
        "scrolled": true,
        "id": "774c7c64"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb5d631e",
      "metadata": {
        "id": "bb5d631e"
      },
      "outputs": [],
      "source": [
        "unique_sequence_labels = list(set(sequence_labels))\n",
        "unique_sequence_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9fce2f",
      "metadata": {
        "id": "3e9fce2f"
      },
      "outputs": [],
      "source": [
        "sequence_labels = [unique_sequence_labels.index(l) for l in sequence_labels]\n",
        "\n",
        "print(f'There are {len(unique_sequence_labels)} unique sequence labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de12c3c",
      "metadata": {
        "id": "6de12c3c"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "\n",
        "unique_token_labels = list(set(reduce(lambda x, y: x + y, labels_for_tokens)))\n",
        "labels_for_tokens = [[unique_token_labels.index(_) for _ in l] for l in labels_for_tokens]\n",
        "\n",
        "print(f'There are {len(unique_token_labels)} unique token labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a5251b7",
      "metadata": {
        "id": "3a5251b7"
      },
      "outputs": [],
      "source": [
        "print(tokenized_utterances[0])\n",
        "print(labels_for_tokens[0])\n",
        "print([unique_token_labels[l] for l in labels_for_tokens[0]])\n",
        "print(utterances[0])\n",
        "print(sequence_labels[0])\n",
        "print(unique_sequence_labels[sequence_labels[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8c1190",
      "metadata": {
        "id": "9f8c1190"
      },
      "outputs": [],
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec863af",
      "metadata": {
        "id": "eec863af"
      },
      "outputs": [],
      "source": [
        "snips_dataset = Dataset.from_dict(\n",
        "    dict(\n",
        "        utterance=utterances, \n",
        "        label=sequence_labels,\n",
        "        tokens=tokenized_utterances,\n",
        "        token_labels=labels_for_tokens\n",
        "    )\n",
        ")\n",
        "\n",
        "snips_dataset = snips_dataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f0d07c",
      "metadata": {
        "id": "a1f0d07c"
      },
      "outputs": [],
      "source": [
        "snips_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8068e6c3",
      "metadata": {
        "id": "8068e6c3"
      },
      "outputs": [],
      "source": [
        "unique_sequence_labels[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a6b70d",
      "metadata": {
        "id": "67a6b70d"
      },
      "outputs": [],
      "source": [
        "snips_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c32ae83",
      "metadata": {
        "id": "6c32ae83"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6302ea14",
      "metadata": {
        "id": "6302ea14"
      },
      "outputs": [],
      "source": [
        "tokenizer('hi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51f3915",
      "metadata": {
        "id": "d51f3915"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode([101, 2603, 1142, 18977, 126, 2940, 102])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d307ac3",
      "metadata": {
        "id": "2d307ac3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc13ba97",
      "metadata": {
        "id": "dc13ba97"
      },
      "outputs": [],
      "source": [
        "# simple function to batch tokenize utterances with truncation\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"utterance\"], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee6381f",
      "metadata": {
        "scrolled": true,
        "id": "bee6381f"
      },
      "outputs": [],
      "source": [
        "seq_clf_tokenized_snips = snips_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da2a0bc",
      "metadata": {
        "id": "3da2a0bc"
      },
      "outputs": [],
      "source": [
        "# only input_ids, attention_mask, and label are used. The rest are for show\n",
        "seq_clf_tokenized_snips['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2333ed",
      "metadata": {
        "id": "9a2333ed"
      },
      "outputs": [],
      "source": [
        "# DataCollatorWithPadding creates batch of data. It also dynamically pads text to the \n",
        "#  length of the longest element in the batch, making them all the same length. \n",
        "#  It's possible to pad your text in the tokenizer function with padding=True, dynamic padding is more efficient.\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b14125",
      "metadata": {
        "id": "93b14125"
      },
      "outputs": [],
      "source": [
        "# Data Collator will pad data so that all examples are the same input length.\n",
        "#  Attention mask is how we ignore attention scores for padding tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154039ff",
      "metadata": {
        "id": "154039ff"
      },
      "outputs": [],
      "source": [
        "sequence_clf_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8703aab3",
      "metadata": {
        "id": "8703aab3"
      },
      "outputs": [],
      "source": [
        "{i: l for i, l in enumerate(unique_sequence_labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d647d950",
      "metadata": {
        "scrolled": true,
        "id": "d647d950"
      },
      "outputs": [],
      "source": [
        "sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-cased', \n",
        "    num_labels=len(unique_sequence_labels),\n",
        ")\n",
        "\n",
        "# set an index -> label dictionary\n",
        "sequence_clf_model.config.id2label = {i: l for i, l in enumerate(unique_sequence_labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4db4d8e",
      "metadata": {
        "id": "a4db4d8e"
      },
      "outputs": [],
      "source": [
        "sequence_clf_model.config.id2label[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6352abe5",
      "metadata": {
        "id": "6352abe5"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):  # custom method to take in logits and calculate accuracy of the eval set\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04da07b",
      "metadata": {
        "id": "a04da07b"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./snips_clf/results\",\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    load_best_model_at_end=True,\n",
        "    \n",
        "    # some deep learning parameters that the Trainer is able to take in\n",
        "    warmup_steps=len(seq_clf_tokenized_snips['train']) // 5,  # number of warmup steps for learning rate scheduler,\n",
        "    weight_decay = 0.05,\n",
        "    \n",
        "    logging_steps=1,\n",
        "    log_level='info',\n",
        "    evaluation_strategy='epoch',\n",
        "    eval_steps=50,\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n",
        "# Define the trainer:\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=sequence_clf_model,\n",
        "    args=training_args,\n",
        "    train_dataset=seq_clf_tokenized_snips['train'],\n",
        "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
        "    compute_metrics=compute_metrics,  # optional\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c7d229",
      "metadata": {
        "id": "d1c7d229"
      },
      "outputs": [],
      "source": [
        "# Get initial metrics\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f1cf8e",
      "metadata": {
        "scrolled": true,
        "id": "08f1cf8e"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8fe9537",
      "metadata": {
        "id": "f8fe9537"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09af3b4e",
      "metadata": {
        "id": "09af3b4e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b373004",
      "metadata": {
        "id": "5b373004"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1a1b76",
      "metadata": {
        "id": "4d1a1b76"
      },
      "outputs": [],
      "source": [
        "pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7d27bf",
      "metadata": {
        "scrolled": true,
        "id": "7d7d27bf"
      },
      "outputs": [],
      "source": [
        "# We can now load our fine-tuned from our directory\n",
        "pipe = pipeline(\"text-classification\", \"./snips_clf/results\", tokenizer=tokenizer)\n",
        "\n",
        "pipe('Please add Here We Go by Dispatch to my road trip playlist')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15302def",
      "metadata": {
        "id": "15302def"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52fef242",
      "metadata": {
        "scrolled": true,
        "id": "52fef242"
      },
      "outputs": [],
      "source": [
        "frozen_sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-cased', \n",
        "    num_labels=len(unique_sequence_labels),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8fa35be",
      "metadata": {
        "id": "e8fa35be"
      },
      "outputs": [],
      "source": [
        "# freezes EVERY parameter in our bert model\n",
        "# does not freeze our classification layer\n",
        "for param in frozen_sequence_clf_model.distilbert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dad2bf7",
      "metadata": {
        "id": "8dad2bf7"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./snips_clf/results\",\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    load_best_model_at_end=True,\n",
        "    \n",
        "    # some deep learning parameters that the Trainer is able to take in\n",
        "    warmup_steps=len(seq_clf_tokenized_snips['train']) // 5,  # number of warmup steps for learning rate scheduler,\n",
        "    weight_decay = 0.05,\n",
        "    \n",
        "    logging_steps=1,\n",
        "    log_level='info',\n",
        "    evaluation_strategy='epoch',\n",
        "    eval_steps=50,\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n",
        "# Define the trainer:\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=frozen_sequence_clf_model,\n",
        "    args=training_args,\n",
        "    train_dataset=seq_clf_tokenized_snips['train'],\n",
        "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc1d2ed0",
      "metadata": {
        "id": "cc1d2ed0"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f09e16e",
      "metadata": {
        "id": "4f09e16e"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b911ff29",
      "metadata": {
        "id": "b911ff29"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b3b0af",
      "metadata": {
        "id": "58b3b0af"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0fae0647",
      "metadata": {
        "id": "0fae0647"
      },
      "source": [
        "## BONUS MATERIAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004a3129",
      "metadata": {
        "id": "004a3129"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9d99579a",
      "metadata": {
        "id": "9d99579a"
      },
      "source": [
        "## BERT for token classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6edfcd",
      "metadata": {
        "id": "bb6edfcd"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification, DistilBertForTokenClassification, \\\n",
        "                         DistilBertTokenizerFast, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cdb1bcc",
      "metadata": {
        "scrolled": true,
        "id": "3cdb1bcc"
      },
      "outputs": [],
      "source": [
        "# using a cased tokenizer because I think case will matter\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "446479c2",
      "metadata": {
        "id": "446479c2"
      },
      "outputs": [],
      "source": [
        "snips_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e652f5",
      "metadata": {
        "id": "e4e652f5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d41d14",
      "metadata": {
        "id": "36d41d14"
      },
      "outputs": [],
      "source": [
        "# The given \"token_labels\" may not match up with the BERT wordpiece tokenization so\n",
        "#  this function will map them to the tokenization that BERT uses\n",
        "#  -100 is a reserved for labels where we do not want to calculate losses so BERT doesn't waste time\n",
        "#  trying to predict tokens like CLS or SEP\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"token_labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:  # Set the special tokens to -100.\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)  # CLS and SEP are labeled as -100\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a7ac1d",
      "metadata": {
        "id": "50a7ac1d"
      },
      "outputs": [],
      "source": [
        "snips_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "832dd8e1",
      "metadata": {
        "id": "832dd8e1"
      },
      "outputs": [],
      "source": [
        "# map our dataset from sequence classification to be for token classification\n",
        "tok_clf_tokenized_snips = snips_dataset.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b31d63",
      "metadata": {
        "id": "d4b31d63"
      },
      "outputs": [],
      "source": [
        "tok_clf_tokenized_snips['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b4c9c4",
      "metadata": {
        "id": "f4b4c9c4"
      },
      "outputs": [],
      "source": [
        "tok_clf_tokenized_snips['train'] = tok_clf_tokenized_snips['train'].remove_columns(\n",
        "    ['utterance', 'label', 'tokens', 'token_labels']\n",
        ")\n",
        "\n",
        "tok_clf_tokenized_snips['test'] = tok_clf_tokenized_snips['test'].remove_columns(\n",
        "    ['utterance', 'label', 'tokens', 'token_labels']\n",
        ")\n",
        "\n",
        "tok_clf_tokenized_snips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f29865e",
      "metadata": {
        "id": "1f29865e"
      },
      "outputs": [],
      "source": [
        "tok_data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b715cf3d",
      "metadata": {
        "scrolled": true,
        "id": "b715cf3d"
      },
      "outputs": [],
      "source": [
        "tok_clf_model = DistilBertForTokenClassification.from_pretrained(\n",
        "    'distilbert-base-cased', num_labels=len(unique_token_labels)\n",
        ")\n",
        "\n",
        "# Set our label dictionary\n",
        "tok_clf_model.config.id2label = {i: l for i, l in enumerate(unique_token_labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bda49b",
      "metadata": {
        "id": "08bda49b"
      },
      "outputs": [],
      "source": [
        "tok_clf_model.config.id2label[0], tok_clf_model.config.id2label[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c848c3f0",
      "metadata": {
        "id": "c848c3f0"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./snips_tok_clf/results\",\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    load_best_model_at_end=True,\n",
        "        \n",
        "    logging_steps=10,\n",
        "    log_level='info',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n",
        "# Define the trainer:\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=tok_clf_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tok_clf_tokenized_snips['train'],\n",
        "    eval_dataset=tok_clf_tokenized_snips['test'],\n",
        "    data_collator=tok_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1c1b92",
      "metadata": {
        "scrolled": true,
        "id": "3c1c1b92"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd4f9bd",
      "metadata": {
        "id": "bdd4f9bd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6cd1c90",
      "metadata": {
        "scrolled": true,
        "id": "f6cd1c90"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b18ae8",
      "metadata": {
        "id": "01b18ae8"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb6a994",
      "metadata": {
        "scrolled": true,
        "id": "8fb6a994"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"token-classification\", tok_clf_model, tokenizer=tokenizer)\n",
        "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6749678a",
      "metadata": {
        "scrolled": true,
        "id": "6749678a"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"token-classification\", tok_clf_model, tokenizer=tokenizer)\n",
        "pipe('Rate the doog food 5 out of 5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4235ced5",
      "metadata": {
        "id": "4235ced5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b414d735",
      "metadata": {
        "id": "b414d735"
      },
      "source": [
        "## Question/Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d54c484d",
      "metadata": {
        "scrolled": true,
        "id": "d54c484d"
      },
      "outputs": [],
      "source": [
        "# From Huggingface: https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad\n",
        "\n",
        "squad_pipe = pipeline(\"question-answering\", \"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b73ece",
      "metadata": {
        "id": "c9b73ece"
      },
      "outputs": [],
      "source": [
        "squad_pipe(\"Where is Sinan living these days?\", \"Sinan lives in California but Matt lives in Boston.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3801222a",
      "metadata": {
        "id": "3801222a"
      },
      "outputs": [],
      "source": [
        "princeton = \"\"\"In 1675, a Quaker missionary from England, encouraged by New Jersey proprietors John Lord \n",
        "              \"Berkeley and Sir George Carteret, arrived to establish a settlement in this area near the \n",
        "              \"Delaware River, which was inhabited by the Lenni-Lenape Indians. The Keith survey of 1685 \n",
        "              \"established the western boundary of Middlesex and Somerset Counties and later, the Township \n",
        "              \"of Princeton. Today Keith's Line is recognized as Province Line Road. With the laying of the \n",
        "              \"cornerstone for Nassau Hall in 1754, Princeton began its development as a location for \n",
        "              \"quality education. Nassau Hall was named for William III, Prince of Orange-Nassau. This simple stone \n",
        "              \"edifice was one of the largest public buildings in the colonies and became a model for many other \n",
        "              \"structures in New Jersey and Pennsylvania.\"\"\"\n",
        "\n",
        "squad_pipe(\"What survey led to the founding of Princeton?\", princeton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79fa03c0",
      "metadata": {
        "id": "79fa03c0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f736e844",
      "metadata": {
        "id": "f736e844"
      },
      "outputs": [],
      "source": [
        "PERSON = 'Sinan Ozdemir'\n",
        "\n",
        "# Note this is NOT an efficient way to search on google. This is done simply for education purposes\n",
        "google_html = BeautifulSoup(requests.get(f'https://www.google.com/search?q={PERSON}').text).get_text()[:512]\n",
        "\n",
        "squad_pipe(f'Who is {PERSON}?', google_html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b79375ac",
      "metadata": {
        "id": "b79375ac"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bee15f2",
      "metadata": {
        "scrolled": true,
        "id": "3bee15f2"
      },
      "outputs": [],
      "source": [
        "# visualize logits\n",
        "large_tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "qa_input = large_tokenizer(  # tokenize our example\n",
        "    \"What survey led to the founding of Princeton?\", princeton,\n",
        "    return_tensors='pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa803db",
      "metadata": {
        "scrolled": true,
        "id": "efa803db"
      },
      "outputs": [],
      "source": [
        "large_qa_bert = AutoModelForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "output = large_qa_bert(**qa_input)  # pass the input through our QA model\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fa3bcd",
      "metadata": {
        "id": "71fa3bcd"
      },
      "outputs": [],
      "source": [
        "token_labels = large_tokenizer.convert_ids_to_tokens(qa_input['input_ids'].squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf6c4ce",
      "metadata": {
        "id": "ddf6c4ce"
      },
      "outputs": [],
      "source": [
        "# Plot start and end logits for our fine-tuned model\n",
        "\n",
        "sns.set(rc={\"figure.figsize\":(20, 5)}) \n",
        "\n",
        "# Create a barplot showing the start word score for all of the tokens.\n",
        "ax = sns.barplot(x=[f'{i} - {t}' for i, t in enumerate(token_labels)], y=output.start_logits.squeeze().tolist(), ci=None)\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "plt.title('Start Word Logits')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Create a barplot showing the start word score for all of the tokens.\n",
        "ax = sns.barplot(x=[f'{i} - {t}' for i, t in enumerate(token_labels)], y=output.end_logits.squeeze().tolist(), ci=None)\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "plt.title('End Word Logits')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c8804d7",
      "metadata": {
        "id": "5c8804d7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008dfd95",
      "metadata": {
        "id": "008dfd95"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}